# Redis 有哪些集群架构模式

## 一、单机版

![redis-redis-单机版](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-redis-%E5%8D%95%E6%9C%BA%E7%89%88.png?raw=true)

**特点：**简单

**存在的问题：**

*1、内存容量有限*

*2、处理能力有限*

*3、无法保证高可用*

## 二、主从复制

主从复制模型中，有多个redis节点。

 其中，**有且仅有**一个为主节点Master。从节点Slave可以有多个。

只要网络连接正常，Master会一直将自己的数据更新同步给Slaves，保持主从同步。



![redis-redis-master-slave](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-redis-master-slave.png?raw=true)

Redis的复制（replication）功能允许用户根据一个redis服务器创建任意多个复制品，在该模式下，多节点部署时，会自动把集群节点区分为主节点（master）和从节点（slave），主从服务器节点上保存着完全一致的全量数据，在网络正常的情况下，master会一直将自身的数据更新同步到slave节点，从而保证主从服务器数据一致性。

主节点（master）：可进行读写操作，主节点可以有多个从节点，一主多从

从节点（slave）：只能读操作，从节点只有一个主节点

> 从数据库连接主数据库，发送 SYNC 命令;
> 主数据库接收到 SYNC 命令后，可以执行 BGSAVE 命令生成 RDB 文件并使用缓冲区记录此后执行的所有写命令;
> 主数据库 BGSAVE 执行完后，向所有从数据库发送快照文件，并在发送期间继续记录被执行的写命令;
> 从数据库收到快照文件后丢弃所有旧数据，载入收到的快照;
> 主数据库快照发送完毕后开始向从数据库发送缓冲区中的写命令;
> 从数据库完成对快照的载入，开始接受命令请求，并执行来自主数据库缓冲区的写命令;(从数据库初始化完成)
> 主数据库每执行一个写命令就会向从数据库发送相同的写命令，从数据库接收并执行收到的写命令(从数据库初始化完成后的操作)
> 出现断开重连后，2.8 之后的版本会将断线期间的命令传给从数据库，增量复制。
> 主从刚刚连接的时候，进行全量同步;全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。Redis 的策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。

**优点：**

> 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离;
> 为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务依然必须由 Master 来完成;
> Slave 同样可以接受其他 Slaves 的连接和同步请求，这样可以有效地分载 Master 的同步压力;
> Master 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求;
> Slave 同样是以阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis 则返回同步之前的数据。

**缺点：**

> Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的 IP 才能恢复;
> 主机宕机，宕机前有部分数据未能及时同步到从机，切换 IP 后还会引入数据不一致的问题，降低了系统的可用性;
> 如果多个 Slave 断线了，需要重启的时候，尽量不要在同一时间段进行重启。因为只要 Slave 启动，就会发送 sync 请求和主机全量同步，当多个 Slave 重启的时候，可能会导致 Master IO 剧增从而宕机。
> Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂;
> redis 的主节点和从节点中的数据是一样的，降低内存的可用性。

**特点：**

（1）主节点Master**可读、可写.**

（2）从节点Slave只读。（read-only）

因此，主从模型可以提高**读的能力**，在一定程度上缓解了写的能力。因为能写仍然只有Master节点一个，可以将读的操作全部移交到从节点上，变相提高了写能力。

## 三、Sentinel 哨兵

![redis-redis-cluster-sentinel](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-redis-cluster-sentinel.png?raw=true)

主从模式下，当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这种方式并不推荐，实际生产中，我们优先考虑哨兵模式。这种模式下，master 宕机，哨兵会自动选举 master 并将其他的 slave 指向新的 master。



在主从模式下，redis 同时提供了哨兵命令`redis-sentinel`，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵进程向所有的 redis 机器发送命令，等待 Redis 服务器响应，从而监控运行的多个 Redis 实例。



哨兵可以有多个，一般为了便于决策选举，使用奇数个哨兵。哨兵可以和 redis 机器部署在一起，也可以部署在其他的机器上。多个哨兵构成一个哨兵集群，哨兵直接也会相互通信，检查哨兵是否正常运行，同时发现 master 宕机哨兵之间会进行决策选举新的 master



Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。

其中三个特性：

**监控（Monitoring）：** Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。

> 1）Sentinel可以监控任意多个Master和该Master下的Slaves。（即多个主从模式）
>
> （2）同一个哨兵下的、不同主从模型，彼此之间相互独立。
>
> （3）Sentinel会不断检查Master和Slaves是否正常。

**提醒（Notification）：** 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。

**自动故障迁移（Automatic failover）：** 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。

> 监控同一个Master的Sentinel会自动连接，组成一个分布式的Sentinel网络，互相通信并交换彼此关于被监视服务器的信息。上图中，三个Sentinel，自动组成Sentinel网络结构。
>
> 疑问：为什么要使用sentinel网络呢？
>
> 答：当只有一个sentinel的时候，如果这个sentinel挂掉了，那么就无法实现自动故障切换了。
>
> 在sentinel网络中，只要还有一个sentinel活着，就可以实现故障切换。

**故障切换的过程：**

> （1）投票**（半数原则）**
>
> 当任何一个Sentinel发现被监控的Master下线时，会通知其它的Sentinel开会，投票确定该Master是否下线（半数以上，所以sentinel通常配奇数个）。
>
> （2）选举
>
> 当Sentinel确定Master下线后，会在所有的Slaves中，选举一个新的节点，升级成Master节点。
>
> 其它Slaves节点，转为该节点的从节点。
>
> （3）原Master重新上线
>
> 当原Master节点重新上线后，自动转为当前Master节点的从节点。

哨兵模式的作用:

- 通过发送命令，让 Redis 服务器返回监控其运行状态，包括主服务器和从服务器;
- 当哨兵监测到 master 宕机，会自动将 slave 切换到 master，然后通过*发布订阅模式*通过其他的从服务器，修改配置文件，让它们切换主机;
- 然而一个哨兵进程对 Redis 服务器进行监控，也可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。



哨兵很像 kafka 集群中的 zookeeper 的功能。



哨兵模式如何工作：

> - 每个 Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的 Master 主服务器，Slave 从服务器以及其他 Sentinel（哨兵）进程发送一个 PING 命令。
> - 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）
> - 如果一个 Master 主服务器被标记为主观下线（SDOWN），则正在监视这个 Master 主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认 Master 主服务器的确进入了主观下线状态
> - 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认 Master 主服务器进入了主观下线状态（SDOWN）， 则 Master 主服务器会被标记为客观下线（ODOWN）
> - 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有 Master 主服务器、Slave 从服务器发送 INFO 命令。
> - 当 Master 主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master 主服务器的所有 Slave 从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。
> - 若没有足够数量的 Sentinel（哨兵）进程同意 Master 主服务器下线， Master 主服务器的客观下线状态就会被移除。若 Master 主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master 主服务器的主观下线状态就会被移除。
>
> 
>
> 假设 master 宕机，sentinel 1 先检测到这个结果，系统并不会马上进行 failover(故障转移)选出新的 master，仅仅是 sentinel 1 主观的认为 master 不可用，这个现象成为**主观下线**。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由 sentinel 1 发起，进行 failover 操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为**客观下线**。这样对于客户端而言，一切都是透明的。

优点

- 哨兵模式是基于主从模式的，所有主从的优点，哨兵模式都具有。
- 主从可以自动切换，系统更健壮，可用性更高。

缺点 

- 具有主从模式的缺点，每台机器上的数据是一样的，内存的可用性较低。
- Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。

特点：

1、保证高可用

2、监控各个节点

3、自动故障迁移

4、读写分离

存在的问题：

主从模式，切换需要时间，期间会丢数据

没有解决 master 写的压力

主节点的存储能力受到单机的限制

通过节点数据冗余，浪费资源

## 四、集群

业界主流的Redis集群化方案主要包括以下几个：

- 客户端分片
- 代理分片
- 服务端分片

代理分片包括：

- Codis
- Twemproxy

服务端分片包括：

- Redis Cluster

它们还可以用是否中心化来划分，其中客户端分片、Redis Cluster属于无中心化的集群方案，Codis、Tweproxy属于中心化的集群方案。

是否中心化是指客户端访问多个Redis节点时，是直接访问还是通过一个中间层Proxy来进行操作，直接访问的就属于无中心化的方案，通过中间层Proxy访问的就属于中心化的方案，它们有各自的优劣，下面分别来介绍。

### 客户端分片

客户端分片主要是说，我们只需要部署多个Redis节点，具体如何使用这些节点，主要工作在客户端。

客户端通过固定的Hash算法，针对不同的key计算对应的Hash值，然后对不同的Redis节点进行读写。

![redis-客户端分片](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%88%86%E7%89%87.png?raw=true)



> 客户端分片集群模式 需要业务开发人员事先评估业务的请求量和数据量，然后让DBA部署足够的节点交给开发人员使用即可。
>
> 这个方案的优点是部署非常方便，业务需要多少个节点DBA直接部署交付即可，剩下的事情就需要业务开发人员根据节点数量来编写key的请求路由逻辑，制定一个规则，一般采用固定的Hash算法，把不同的key写入到不同的节点上，然后再根据这个规则进行数据读取。
>
> 可见，它的缺点是业务开发人员使用Redis的成本较高，需要编写路由规则的代码来使用多个节点，而且如果事先对业务的数据量评估不准确，后期的扩容和迁移成本非常高，因为节点数量发生变更后，Hash算法对应的节点也就不再是之前的节点了。
>
> 所以后来又衍生出了一致性哈希算法，就是为了解决当节点数量变更时，尽量减少数据的迁移和性能问题。
>
> 这种客户端分片的方案一般用于业务数据量比较稳定，后期不会有大幅度增长的业务场景下使用，只需要前期评估好业务数据量即可。

**特点：**

> 这实际上是一种静态分片技术。Redis 实例的增减，都得手工调整分片程序。基于此分片机制的开源产品，现在仍不多见。
>
> 这种分片机制的性能比代理式更好（少了一个中间分发环节）。但缺点是升级麻烦，对研发人员的个人依赖性强——需要有较强的程序开发能力做后盾。如果主力程序员离职，可能新的负责人，会选择重写一遍。
>
> 所以，这种方式下，可运维性较差。出现故障，定位和解决都得研发和运维配合着解决，故障时间变长。
>
> 这种方案，难以进行标准化运维，不太适合中小公司（除非有足够的 DevOPS）。

### 代理分片

将分片工作交给专门的代理程序来做。代理程序接收到来自业务程序的数据请求，根据路由规则，将这些请求分发给正确的 Redis 实例并返回给业务程序。

![redis-代理分片](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-%E4%BB%A3%E7%90%86%E5%88%86%E7%89%87.png?raw=true)

该方案下，一般会选用第三方代理程序（而不是自己研发），因为后端有多个 Redis 实例，所以这类程序又称为分布式中间件。

这样的好处是，业务程序不用关心后端 Redis 实例，运维起来也方便。虽然会因此带来些性能损耗，但对于 Redis 这种内存读写型应用，相对而言是能容忍的。

#### Codis

Codis是由国人前豌豆荚大神开发的，采用中心化方式的集群方案。因为需要代理层Proxy来进行所有请求的转发，所以对Proxy的性能要求很高，Codis采用Go语言开发，兼容了开发效率和性能。

Codis包含了多个组件：

- codis-proxy：主要负责对请求的读写进行转发
- codis-dashbaord：统一的控制中心，整合了数据转发规则、故障自动恢复、数据在线迁移、节点扩容缩容、自动化运维API等功能
- codis-group：基于Redis 3.2.8版本二次开发的Redis Server，增加了异步数据迁移功能
- codis-fe：管理多个集群的UI界面

可见Codis的组件还是挺多的，它的功能非常全，除了请求转发功能之外，还实现了在线数据迁移、节点扩容缩容、故障自动恢复等功能。

Codis的Proxy就是负责请求转发的组件，它内部维护了请求转发的具体规则，Codis把整个集群划分为1024个槽位，在处理读写请求时，采用`crc32`Hash算法计算key的Hash值，然后再根据Hash值对1024个槽位取模，最终找到具体的Redis节点。

Codis最大的特点就是可以在线扩容，在扩容期间不影响客户端的访问，也就是不需要停机。这对业务使用方是极大的便利，当集群性能不够时，就可以动态增加节点来提升集群的性能。

为了实现在线扩容，保证数据在迁移过程中还有可靠的性能，Codis针对Redis进行了修改，增加了针对异步迁移数据相关命令，它基于Redis 3.2.8进行开发，上层配合Dashboard和Proxy组件，完成对业务无损的数据迁移和扩容功能。

因此，要想使用Codis，必须使用它内置的Redis，这也就意味着Codis中的Redis是否能跟上官方最新版的功能特性，可能无法得到保障，这取决于Codis的维护方，目前Codis已经不再维护，所以使用Codis时只能使用3.2.8版的Redis，这是一个痛点。

另外，由于集群化都需要部署多个节点，因此操作集群并不能完全像操作单个Redis一样实现所有功能，主要是对于操作多个节点可能产生问题的命令进行了禁用或限制，具体可参考Codis不支持的命令列表。

#### Twemproxy

Twemproxy是由Twitter开源的集群化方案，它既可以做Redis Proxy，还可以做Memcached Proxy。

它的功能比较单一，只实现了请求路由转发，没有像Codis那么全面有在线扩容的功能，它解决的重点就是把客户端分片的逻辑统一放到了Proxy层而已，其他功能没有做任何处理。



Tweproxy推出的时间最久，在早期没有好的服务端分片集群方案时，应用范围很广，而且性能也极其稳定。

但它的痛点就是无法在线扩容、缩容，这就导致运维非常不方便，而且也没有友好的运维UI可以使用。

Twemproxy 是一个 Twitter 开源的一个 redis 和 memcache 快速/轻量级代理服务器； Twemproxy 是一个快速的单线程代理程序，支持 Memcached ASCII 协议和 redis 协议。

特点：

1、多种 hash 算法：MD5、CRC16、CRC32、CRC32a、hsieh、murmur、Jenkins

2、支持失败节点自动删除

3、后端 Sharding 分片逻辑对业务透明，业务方的读写方式和操作单个 Redis 一致

缺点：

增加了新的 proxy，需要维护其高可用。failover 逻辑需要自己实现，其本身不能支持故障的自动转移可扩展性差，进行扩缩容都需要手动干预

Codis就是因为在这种背景下才衍生出来的。

### 服务端分片



#### Redis Cluster

![redis-Redis3.0 cluster集群模式](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-Redis3.0%20cluster%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F.png?raw=true)

Redis官方推出的Redis Cluster另辟蹊径，它没有采用中心化模式的Proxy方案，而是把请求转发逻辑一部分放在客户端，一部分放在了服务端，它们之间互相配合完成请求的处理。

[Redis Cluster](http://mp.weixin.qq.com/s?__biz=MzI3ODcxMzQzMw%3D%3D&chksm=eb538b23dc24023586dd2ee445c4b357c1b7868f9c428856d3426336d10464b6be7ea67831cc&idx=1&mid=2247486997&scene=21&sn=bb2cd6b017f5dc9c79ded3776e3ffdea#wechat_redirect)是在Redis 3.0推出的，早起的Redis Cluster由于没有经过严格的测试和生产验证，所以并没有广泛推广开来。也正是在这样的背景下，业界衍生了出了上面所说的中心化集群方案：Codis和Tweproxy。

但随着Redis的版本迭代，Redis官方的Cluster也越来越稳定，更多人开始采用官方的集群化方案。也正是因为它是官方推出的，所以它的持续维护性可以得到保障，这就比那些第三方的开源方案更有优势。

Redis Cluster没有了中间的Proxy代理层，那么是如何进行请求的转发呢？

Redis把请求转发的逻辑放在了Smart Client中，要想使用Redis Cluster，必须升级Client SDK，这个SDK中内置了请求转发的逻辑，所以业务开发人员同样不需要自己编写转发规则，Redis Cluster采用16384个槽位进行路由规则的转发。

![redis- cluster-分片机制-虚拟槽](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-%20cluster-%E5%88%86%E7%89%87%E6%9C%BA%E5%88%B6-%E8%99%9A%E6%8B%9F%E6%A7%BD.png?raw=true)

> 没有了Proxy层进行转发，客户端可以直接操作对应的Redis节点，这样就少了Proxy层转发的性能损耗。
>
> Redis Cluster也提供了在线数据迁移、节点扩容缩容等功能，内部还内置了哨兵完成故障自动恢复功能，可见它是一个集成所有功能于一体的Cluster。因此它在部署时非常简单，不需要部署过多的组件，对于运维极其友好。
>
> Redis Cluster在节点数据迁移、扩容缩容时，对于客户端的请求处理也做了相应的处理。当客户端访问的数据正好在迁移过程中时，服务端与客户端制定了一些协议，来告知客户端去正确的节点上访问，帮助客户端订正自己的路由规则。
>
> 虽然Redis Cluster提供了在线数据迁移的功能，但它的迁移性能并不高，迁移过程中遇到大key时还有可能长时间阻塞迁移的两个节点，这个功能相较于Codis来说，Codis数据迁移性能更好。
>
> 现在越来越多的公司开始采用Redis Cluster，有能力的公司还在它的基础上进行了二次开发和定制，来解决Redis Cluster存在的一些问题，我们期待Redis Cluster未来有更好的发展。
>
> 特点：
>
> 1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。
>
> 2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。
>
> 3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。
>
> 4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本
>
> 5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave到 Master 的角色提升。
>
> 缺点：
>
> 1、资源隔离性较差，容易出现相互影响的情况。
>
> 2、数据通过异步复制,不保证数据的强一致性



### 1.redis cluster为什么没有使用一致性hash算法，而是使用了哈希槽预分片？ 

>  缓存热点问题：一致性哈希算法在节点太少时，容易因为数据分布不均匀而造成缓存热点的问题。一致性哈希算法可能集中在某个hash区间内的值特别多，会导致大量的数据涌入同一个节点，造成master的热点问题(如同一时间20W的请求都在某个hash区间内)。

### 2.redis的hash槽为什么是16384(2^14)个卡槽，而不是65536(2^16)个？

> 1. **心跳包可能会太大导致网络阻塞**。redis 集群是使用 gossip 流言协议，类似于病毒扩散的形式传播各个节点的信息达到最终一致的。假设槽位设置到了最大，如果一个节点刚好都用到了这些槽位，会导致心跳包达到了 8K，在集群内如果进行传播会导致网络阻塞（1W6 转换成心跳包的大小刚好是 2K，拉满哈希槽的时候就是 8K）
> 2. **集群总节点数在大多数场景不会超过 1K个**。假设在极端的场景有 1K 个节点，1W6 的哈希槽位也足以应对
> 3. **提高 gossip 流言协议的传播效率**。每个节点都会保存自己用到的哈希槽的 bitmap，在基于流言协议大范围传播节点信息时，会「哈希槽/节点数」的压缩，如果槽位很多的话压缩比率就高，传播节点信息的效率就会高起来

### 3.集群配置

> 1.redis cluster的集群模式可以部分提供服务，当redis.conf的配置cluster-require-full-coverage为no时，表示当一个小主从整体挂掉的时候集群也可以用，也是说0-16383个槽位中，落在该主从对应的slots上面的key是用不了的，但key落在其他的范围是仍然可用的。
>
> 2.在cluster架构下，默认的，一般redis-master用于接收读写，而redis-slave则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。
>
> 3.redis-cluster 不可用的情况
> （1）集群主库半数宕机（无论是否从库存活）。
> （2）集群某一节点的主从全数宕机。

### 4.Redis Cluster 集群伸缩原理

> Redis集群中的每个node(节点)负责分摊这16384个slot中的一部分，也就是说，每个slot都对应一个node负责处理。当动态添加或减少node节点时，只需要将16384个槽做个再分配，将槽中的键值和对应的数据迁移到对应的节点上。redis cluster提供了灵活的节点扩容和收缩方案。在不影响集群对外服务的情况下，可以为集群添加节点进行扩容，也可以下线部分节点进行缩容。可以说，槽是 Redis 集群管理数据的基本单位，集群伸缩就是槽和数据在节点之间的移动。



#### (1)集群扩容

> 当一个 Redis 新节点运行并加入现有集群后，我们需要为其迁移槽和槽对应的数据。首先要为新节点指定槽的迁移计划，确保迁移后每个节点负责相似数量的槽，从而保证这些节点的数据均匀。如下图：向有三个master集群中加入M4(即node-4)，集群中槽和数据的迁移。

![](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-cluster-%E9%9B%86%E7%BE%A4%E6%89%A9%E5%AE%B9.png?raw=true)

> **集群扩容过程：**
>
> 1.首先启动一个 Redis 节点，记为 M4。
>
> 2.使用 cluster meet 命令，让新 Redis 节点加入到集群中。新节点刚开始都是主节点状态，由于没有负责的槽，所以不能接受任何读写操作，后续我们就给他迁移槽和填充数据。
>
> 3.对M4节点发送 cluster setslot { slot } importing { sourceNodeId } 命令，让目标节点准备导入槽的数据。 对源节点，也就是 M1，M2，M3 节点发送 cluster setslot { slot } migrating { targetNodeId } 命令，让源节点准备迁出槽的数据。
>
> 4.源节点执行 cluster getkeysinslot { slot } { count } 命令，获取 count 个属于槽 { slot } 的键，然后执行步骤五的操作进行迁移键值数据。
>
> 5.在源节点上执行 migrate { targetNodeIp} " " 0 { timeout } keys { key... } 命令，把获取的键通过 pipeline 机制批量迁移到目标节点，批量迁移版本的 migrate 命令在 Redis 3.0.6 以上版本提供。
>
> 6.重复执行步骤 5 和步骤 6 直到槽下所有的键值数据迁移到目标节点。
>
> 7.向集群内所有主节点发送 cluster setslot { slot } node { targetNodeId } 命令，通知槽分配给目标节点。为了保证槽节点映射变更及时传播，需要遍历发送给所有主节点更新被迁移的槽执行新节点



#### (2)集群收缩

>收缩节点就是将 Redis 节点下线，整个流程需要如下操作流程：
>
>- 首先需要确认下线节点是否有负责的槽，如果有，需要把槽和对应的数据迁移到其它节点，保证节点下线后整个集群槽节点映射的完整性。
>- 当下线节点不再负责槽或者本身是从节点时，就可以通知集群内其他节点忘记下线节点，当所有的节点忘记该节点后可以正常关闭。
>
>下线节点需要将节点自己负责的槽迁移到其他节点，原理与之前节点扩容的迁移槽过程一致。迁移完槽后，还需要通知集群内所有节点忘记下线的节点，也就是说让其它节点不再与要下线的节点进行 Gossip 消息交换。

![](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-cluster-%E9%9B%86%E7%BE%A4%E7%BC%A9%E5%AE%B9.png?raw=true)

### 5.客户端请求路由

#### （1）moved重定向

![redis-client请求路由](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-client%E8%AF%B7%E6%B1%82%E8%B7%AF%E7%94%B1.png?raw=true)

客服端请求产生moved重定向的执行过程：

- 1.每个节点通过通信都会共享Redis Cluster中槽和集群中对应节点的关系。
- 2.客户端向Redis Cluster的任意节点发送命令，接收命令的节点会根据CRC16规则进行hash运算与16383取余，计算自己的槽和对应节点 。
- 3.如果保存数据的槽被分配给当前节点，则去槽中执行命令，并把命令执行结果返回给客户端。
- 4.如果保存数据的槽不在当前节点的管理范围内，则向客户端返回moved重定向异常 。
- 5.客户端接收到节点返回的结果，如果是moved异常，则从moved异常中获取目标节点的信息。
- 6.客户端向目标节点发送命令，获取命令执行结果。

#### （2）ask重定向

在对集群进行扩容和缩容时，需要对槽及槽中数据进行迁移。当槽及槽中数据正在迁移时，客服端请求目标节点时，目标节点中的槽已经迁移到别的节点上了，此时目标节点会返回ask转向给客户端。

当客户端向某个节点发送命令，节点向客户端返回moved异常，告诉客户端数据对应的槽的节点信息；客户端再向正确的节点发送命令时，如果此时正在进行集群扩展或者缩空操作，槽及槽中数据已经被迁移到别的节点了，就会返回ask，这就是ask重定向机制。如下图：

图推图u图图

**请求执行步骤：**

- 1.当客户端向集群中某个节点发送命令，节点向客户端返回moved异常，告诉客户端数据对应目标槽的节点信息。
- 2.客户端再向目标节点发送命令，目标节点中的槽已经迁移出别的节点上了，此时目标节点会返回ask重定向给客户端。
- 2.客户端向新的target节点发送Asking命令，然后再次向新节点发送请求请求命令。
- 3.新节点target执行命令，把命令执行结果返回给客户端。

> moved和ask重定向的区别：
> 两者都是客户端重定向
> moved异常：槽已经确定迁移，即槽已经不在当前节点
> ask异常：槽还在迁移中

#### （3）Smart客户端

当数据过多，集群节点较多时，客服端大多数请求都会发生重定向，每次重定向都会产生一次无用的请求，严重影响了redis的性能。如果客户端在请求时就知道由哪个节点负责管理哪个槽，再将请求打到对应的节点上，那就有效的解决了这个问题。

提高redis的性能，避免大部分请求发生重定向，可以使用**智能客户端**。智能客户端知道由哪个节点负责管理哪个槽，而且某个当节点与槽的映射关系发生改变时，客户端也会进行响应的更新，这是一种非常高效的请求方式。

Jedis为Redis Cluster提供了Smart客户端，也就是JedisCluster类。JedisCluster会从集群中选一个可运行的节点，使用 cluster slots 初始化槽和节点映射，将映射关系保存到本地，为每个节点创建JedisPool，相当于为每个redis节点都设置一个JedisPool，然后就可以进行数据读写操作。

![](https://github.com/haoyutc/haoyutc.github.io/blob/master/images/redis-smart%E5%AE%A2%E6%88%B7%E7%AB%AF.png?raw=true)

**smart智能客户端读写数据命令执行过程如下：**

- 1.JedisCluster启动时，会从集群中选一个可运行的节点，使用 cluster slots 初始化槽和节点映射，将映射关系保存到本地。
- 2.smart 客户端将请求要操作的 key 发送到目标节点，如果请求成功，就得到响应，并返回结果。
- 3.如果目标节点出现连接出错(说明节点的slot->node的映射有更新)，客户端将随机找个活跃节点，向其发送命令，大概率会得到 moved异常，然后根据moved响应更新 slot 和 node 的映射关系，再向新的目标节点发送命令。
- 如果这样的情况连续出现 5 次未找到目标节点，则抛出异常：Too many cluster redirection！。

**总结：**mart智能客户的目标：追求性能。避免了大量请求的moved重定向操作，在数据量和请求量大的环境下，极高的提升了redis性能。

### 6.Redis Cluster主从选举

**当某个master挂掉后，在cluster集群仍然可用的前提下，由于某个master可能有多个slave，某个salve将提升为master节点，那么就会存在竞争，那么此时它们的选举机制是怎样的呢？**

> currentEpoch选举轮次标记 一个集群状态的相关概念，记录集群状态变更的递增版本号。集群中每发生一次master选举currentEpoch就加一，集群节点创建时，不管是 master还是slave，都置currentEpoch为0。
> 当前节点在接受其他节点发送的请求时，如果发送者的currentEpoch（消息头部会包含发送者的 currentEpoch）大于当前节点的currentEpoch，那么当前节点会更新currentEpoch。
> 因此，集群中所有节点的 currentEpoch最终会达成一致，相当于对集群状态的认知达成了一致。

**master节点选举过程：**

- 1.slave发现自己的master变为FAIL。
- 2.发起选举前，slave先给自己的epoch（即currentEpoch）加一，然后请求集群中其它master给自己投票，并广播信息给集群中其他节点。
- 3.slave发起投票后，会等待至少两倍NODE_TIMEOUT时长接收投票结果，不管NODE_TIMEOUT何值，也至少会等待2秒。
- 4.其他节点收到该信息，只有master响应，判断请求者的合法性，并发送结果。
- 5.尝试选举的slave收集master返回的结果，收到超过半投票数master的统一后变成新Master，如果失败会发起第二次选举，选举轮次标记+1继续上面的流程。
- 6.选举成功后，广播Pong消息通知集群其它节点。

> 之所以强制延迟至少0.5秒选举，是为确保master的fail状态在整个集群内传开，否则可能只有小部分master知晓，而master只会给处于fail状态的master的slaves投票。
> 如果一个slave的master状态不是fail，则其它master不会给它投票，Redis通过八卦协议（即Gossip协议，也叫谣言协议）传播fail。
> 而在固定延迟上再加一个随机延迟，是为了避免多个slaves同时发起选举。
>
> #延迟计算公式： DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms
>
> SLAVE_RANK表示此slave已经从master复制数据的总量的rank。Rank越小代表已复制的数据越新。这种方式下，持有最新数据的slave将会首先发起选举（理论上）。



## References

[1]: https://www.huaweicloud.com/articles/38e2316d01880fdbdd63d62aa26b31b4.html
[2]: https://www.cnblogs.com/crazymakercircle/p/14282108.html#autoid-h2-0-0-0
[3]: http://www.ttlsa.com/redis/redis-cluster-theoretical-knowledge/

